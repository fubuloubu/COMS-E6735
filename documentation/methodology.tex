\subsection{Overview}
The \project Project has many layers that must come together to obtain the desired result.
The following is a description of these layers.
\subsubsection{Guitar Localization}
The first layer is to simply identify the location of the guitar within the video frame,
as well as the location of the hands on the guitar.
In practice however this is not an easy task.
The first aspect of this is to 
This will be done using a Binary Detector matching algorithm against a stock photo of the guitar
in order to identify a match within each frame. \cite{opencv}.
\par
Since there exists a nearly endless quantity of not only guitar shapes, body styles, colors, lengths, etc.
that suites an individual owner's tastes (possibly more variability than even the owners themselves!),
the use of the Binary Detector algorithm will be more accurate than using a trained Cascade Classifier,
as was attempted originally.
The algorithm could be refined such that a set of generic descriptor features are identified so
this layer can identify many different styles for a type of guitars.
For now, the plan is to accept as input a stock photo of the guitar that is to be identified.
\par
Not only does the guitar localization layer need to recognize these guitars,
but will also need to recognize these guitars in context of the players who play them.
This means the ability to recognize the guitar despite player's hands and potentially clothing obscuring
features of the guitar that may help to locate it.
Additionally, the location of the hands can vary drastically as the player moves up and down the neck,
and different picking/plucking/strumming techniques at different orientations
means this layer has exponentially more factors to account for.
This means that the Detector algorithm must define use sufficient amount of detectors features
such that the overlaid hands and clothing do not interfere with the operation of the Detector.

\subsubsection{Hand Localization}
The next layer is to find the location of both fretting hand and plucking/picking/strumming hand in context of the guitar.
This layer can be simplistic since the hands are easy to find
i.e. we know that one hand must be located on or near the fretboard and the other on or near the body of the guitar.
Several shortcuts could be taken to isolate the location of the hands by parsing the guitar orientation
and cutting the frame in half along the body/neck joint to segment the search along those two distinct parts.
\par
A standard skin detection filter will be employeed leveraging the segmented guitar location in order to bound
the two regions of skin that define the hand locations on the guitar.

\subsubsection{Guitar Modeling}
The next stage of the overall algorithm is to create a model of the relevant aspects of the guitar we are interested in.
This means we need to find the frets and strings of the guitar, and label them appropiately in context of the guitar.
Specifically, we are looking to identify the string number (string 1, 2, 3, etc. corresponding to E, A, D, etc.)
and fret number (frets 0, 1, 2, 3, etc. leveraging the nut and fret markers available on that specific guitar) and
their intersections, as well as where the strings cross the picking area for our guitar model to be complete enough
for us to indentify notes.
\par
Although this can theoretically be done only once per video given the guitar is in a standard location, 
the reality is that performers tend to move the instrument at least subtlely while performing,
and additionally move their hands significantly which obscures parts of the regions of interest,
so the guitar model will need to be partially estimated until data is available to correct the obscured features.
Thankfully, these locations and spacings are about the only standardized attributes of a guitar,
so this can be done with minimal hassle given a sufficient amount of information in context of the lighting
and occlusion of the instrument in the frame, once the guitar has been successfully and fully localized.
\par
The frets and strings will be identified using a threshold technique which leverages the fact that
(for at least the target bass guitar owned by the player) these features are shiny and reflect
the light sources much more significantly than the guitar body and neck.
Once the thresholding was applied, Hough Line detection (using a relatively large threshold)
is employed in order to isolate the sharply defined lines of the strings from the rest of the picture.
A group of lines all going in the same direction that are of this larger size are then fed into a clustering algorithm
used to find the final amount of matching strings. A Jenks natural breaks algorithm is used to cluster these
lines by their perpindicular distance (using the average angle of these lines), and the resulting groups
are grouped into one representative line for that string.
\par
Once the strings are isolated and determined to be correct, the next stage is to find the frets.
The frets are harder to locate given that they are partly occluded by the strings and that they are
much shorter in length. However, we can leverage the fact that they are perpindicular to the strings
and leverage this fact to limit the Hough Line detection algorithm using a shorter threshold for the lines
by limiting only to the lines in that perpindicular direction and bounded inside a volume occupied by the strings
(with sufficient buffer). Using these restrictions as well as mathematical models for the ideal locations of
the frets, fret detection has moderately accurate results.
\par
These techniques proves fairly reliable in early trials, however lighting and orientation
differences could lead to this algorithm to be somewhat fickle in identifying all of the frets.
Since the guitar should not change scale in frame too much, and the range of orientation
and lighting should be small, the idea was to continually estimate the location of each feature
(string/fret) until a model is built that fully matches the expected number of strings and frets.
This fully realized model could then be tracked for scale and orientation changes in order to
accurately track where the guitar is moving during playing.
The end product could leverage this by instructing a live user to move around their hands until
the guitar model layer has successfully accomplished this, and inform the user when it is unable
to track these quantities during operation.

\subsubsection{Hand Modeling}
From the hand location stage we now need to identify the players fingering and plucking points
by fitting a simplified hand model to each hand image obtained prior and obtaining the interesting fingertips.
For the purposes of this project, we will only consider finger plucking on the right hand of the player
since the bass guitar is the only instrument under consideration
(this only works for right handed guitar players, although a simple mirror operation could possibly correct this easily).
Additionally, this means we are only interested in two points of the plucking hand
(index and middle finger, only two finger plucking techniques will be considered)
and four points of the fretting hand (no thumb fretting techniques will be considered) at this layer.
\par
The method employed here consisted of isolating the skin-tone portion of the cropped hand images \cite{pyimagesearch,seereality}
and passing them through a "skeletonization" filter \cite{skeleton} that blurs and subtracts the background (which is black)
inwards towards the center portion of the image until only a thin, strong line representing the fingers are left.
Hough Line detection was employed at this point in order to indentify line endings in an attempt to find the finger tip locations.
\par
A previous method that was intended to be adapted from several papers concerning the real-time tracking of hands and fingers
\cite{aslhand,handposes,fingertracking}.
However, the lack of available source code and complexity of the algorithms necessitated the simpler approach employed here.

\subsubsection{Note Production Music Model}
The final step is to interpret the hand models in context of the guitar models and determine when a plucking event occurs.
Several techniques may be employed here including frame averaging of these events to ensure that a plucking event
is correctly determined.
At this time, a several frame averaging algorithm will be employed leveraging the tempo of plucking events in context of 
the overall time signature determined by analysis of the events, with care taken to ensure abrupt time signature changes
are tracked accurately.
This will also ensure that the human-readable output of this final layer makes sense in context of the music that it is
analyzing, such that accuracy of the results obtained are at a level as to be useful to the end-user.
The output at this layer will either be directly printed on the screen (as a rolling queue of note events),
or piped to standard output such that the user can collect these events into a tablature file.

\subsection{Development Environment}
Initial prototyping work will be done using Python and OpenCV.
The layers are defined as individual modules such that accuracy of each individual stage can be verified as the design progresses.
Additional training modules for the Cascade Classifiers are located in an adjacent environment
and links to the XML output are maintained such that these classifiers are a part of the configuration of the final product.
As the project progresses an implementation in C++ will be done in order to substantially optimize the processing of the utility.
The performance goal for this application would be on order of the running time of the video.
A system that can operate in real time would be considered succesful and open up a larger array of possibilities and use cases.
%\par
%The system will be constructed as a command line utility;
%either operating on an input video file of a specific format, with the option to
%print out the resultant tabulature to standard output (which may be piped to a file)
%or printing in-frame for use as a teaching aid.
%For verification, the obtained output of the program using the standard output method
%will be saved to a file and compared using an algorithm that will compare
%to a corresponding desired output file that will be constructed by hand for the input video.

\subsection{Evaluation}
Results will be prototyped and verified via a small collection of ~1-2 min video files
(50 total) with minor differences in lighting, clothing choice, and playing style.
%The system will be vetted under a few different circumstances:
%standard performer, lighting, and instruments (20),
%standard enviroment with different fretting techniques (10),
%different lighting conditions (10),
%different instruments (5),
%and different performers (5).
%The system will be designed to adequetely handle these different circumstances
%without requiring enviroment-specific processing directives,
%except in the case of the different instruments/performers,
%where application-specific classifier files will be used.
\par
Each intermediate layer of the overall program will be vetted using simple criteria
for accuracy, which are as follows:
\begin{itemize}
    \item Location Classifiers: Presence of located object
    \item Guitar Model: Number of identified strings and frets
    \item Hand Model: Number of identified finger tips
\end{itemize}
Additionally, manual verification and quality determinations will be made by the author by reviewing
the quality of each layer using visual representations of the processing performed by each layer against the
set of videos provided.
\par
For the final system, an extended verification methodology will be employed against the desired output via the following:
\begin{itemize}
    \item Each note occurence in the desired output will be matched to a corresponding note in the obtained output
    \item If an incorrect note occurs in the obtained output, the algorithm will check ahead to see if further
        matches exist.
    \item When two sections of correct notes occur surrounding a section of incorrect notes, those will
        be analyzed to obtain the difference (missing note, added note or wrong note).
    \item The output correctness factor will be number of matches minus the number of additions and misses
        over the percentage of notes in the desired output (floored at zero percent correct).
\end{itemize}
The video files will be reviewed prior to testing and the desired output will be synthesized by hand.
\par
The overall goal for correctness would be 90\%.
This is an agressive goal for a computer vision application,
especially for one measuring the location of fingers on a hand.
Additionally, the obtained final tablature file can be converted to midi representation
for qualitative feedback using a conversion utility such as \cite{8notes}.
