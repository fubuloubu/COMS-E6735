\subsection{Overview}
The \project Project has many layers that must come together to obtain the desired result.
The following is a description of these layers.
\subsubsection{Guitar Classification}
The first layer is to simply identify the location of the guitar within the video frame,
as well as the location of the hands on the guitar.
In practice however this is not an easy task.
The first aspect of this is to train a Cascade Classifier that can accurately find a pre-defined guitar.
Cascade Classifiers are very susceptible to their input training images,
which means an accurate classifier has to have many depictions of the objects
for it's inteded usage scenario as possible \cite{opencv,codingrobin}.
Since there exists a nearly endless quantity of not only guitar shapes, body styles, colors, lengths, etc.
that suites an individual owner's tastes (possibly more variability than even the owners themselves!),
it is necessary to severely limit the guitars that are able to be detected by this layer.
In particular due to time constraints, it is necessary to limit this to 1 specific guitar.
Future projects can train better classifiers by adding more types, shapes, and colors of guitars,
or otherwise diversify this layer using other techniques.
\par
Not only does the guitar classification layer need to recognize these guitars,
but will also need to recognize these guitars in context of the players who play them.
This means the ability to recognize the guitar despite player's hands and potentially clothing obscuring
features of the guitar that may help to classify it.
Additionally, the location of the hands can vary drastically as the player moves up and down the neck,
and different picking/plucking/strumming techniques at different orientations can make this layer have exponentially more factors to train for.
In this project, this was done by only training with images that contain the specific player's hands
and a specific clothing choice along with the specific guitar the player would be using.
Again, future projects can look into diversifying this layer to account for all manner of player's
skin color, clothing choices, playing styles and other differences that could affect classification.
Truly, this layer has a significant depth to it that could be explored in a separate project in it's own right.
\par
The classifier input images were created using recorded video files:
one set for positive images and one set for negative images.
The positive video file contained the particular player playing the particular guitar
and wearing the particular clothes intended for the final classifier to match against.
The images were parsed by every Nth spaced frame (N = 10 was chosen)
and each frame was manually reviewed for appropiateness and removed if necessary
(about 40-45 positive images were chosen for each classifier).
Those images were then cropped such that the guitar was the main component in the image,
then fed to the sample generator to generate positive samples
(10 randomized samples for each frame were created).
The negative video was taken of the background with the particular player both in the scene
and not in the scene.
In particular, the player was instructed to play air guitar such that the hand positions
without the guitar could be simulated as being part of the negatives.
The images were parsed out of the video file by every Nth spaced frame (N=4) in order to generate
a similar number of negative examples to the generated positive examples

\subsubsection{Hand Classification}
The next layer is to find the location of both fretting hand and plucking/picking/strumming hand in context of the guitar.
Again, we are going to leverage the OpenCV Cascade Classifier trained using the specific player's hands overlaid on the
specific guitar that player is to be playing.
This classifier can be a bit more simplistic however since the context we are using this 
within has much less variability then the guitar layer, 
i.e. we know that the hands must be located on or near the fretboard and on or near the body of the guitar.
Several shortcuts could be taken to isolate the location of the hands by parsing the guitar orientation
and cutting the frame in half along the body/neck joint to segment the search along those two distinct parts.
However the choice was made not to do this at this layer since the fretting and picking hands have distinctly
different shapes that should be make classification between the two possible.
\par
The hand classifier input images were taken from the guitar training files.
The positive images were manually cropped in each frame from the positive examples of the guitar classifier.
The negative images were negative images from the guitar classifier, plus a few extra frames where
the specific guitar was shown without the hand(s) present.
A similar number of examples was generated in comparision to the guitar classifier,
but less stages were employeed.
Additionally, a variety of hand orientations were chosen.

\subsubsection{Guitar Modeling}
The next stage of the overall algorithm is to create a model of the relevant aspects of the guitar we are interested in.
This means we need to find the frets and strings of the guitar, and label them appropiately in context of the guitar.
Specifically, we are looking to identify the string number (string 1, 2, 3, etc. corresponding to E, A, D, etc.)
and fret number (frets 0, 1, 2, 3, etc. leveraging the nut and fret markers available on that specific guitar) and
their intersections, as well as where the strings cross the picking area for our guitar model to be complete enough
for us to indentify notes.
\par
Although this can theoretically be done only once per video given the guitar is in a standard location, 
the reality is that performers tend to move the instrument at least subtlely while performing,
and additionally move their hands significantly which obscures parts of the regions of interest,
so the guitar model will need to be partially estimated until data is available to correct the obscured features.
Thankfully, these locations and spacings are about the only standardized attributes of a guitar,
so this can be done with minimal hassle given a sufficient amount of information in context of the lighting
and occlusion of the instrument in the frame, once the guitar has been corrected to a standard location.
\par
The frets and strings were identified using a threshold technique leveraging the fact that
(for at least the target bass guitar owned by the player) these features are shiny and reflect
the light sources much more significantly than the guitar body and neck.
This technique proved fairly reliable in early trials, however lighting and orientation
differences could lead to this detection to be somewhat fickle in identifying all of the frets.
Since the guitar should not change scale in frame too much, and the range of orientation
and lighting should be small, the idea was to continually estimate the location of each feature
(string/fret) until a model is built that fully matches the expected number of strings and frets.
This fully realized model could then be tracked for scale and orientation changes in order to
accurately track where the guitar is moving during playing.
The end product could leverage this by instructing a live user to move around their hands until
the guitar model layer has successfully accomplished this, and inform the user when it is unable
to track these quantities during operation.

\subsubsection{Hand Modeling}
From the hand classification stage we now need to identify the players fingering and plucking points
by fitting a simplified hand model to each hand image obtained prior.
For the purposes of this project, we will only consider finger plucking on the right hand of the player
since the bass guitar is the only instrument under consideration
(this only works for right handed guitar players, although a simple mirror operation could possibly correct this easily).
Additionally, this means we are only interested in two points of the plucking hand
(index and middle finger, only two finger plucking techniques will be considered)
and four points of the fretting hand (no thumb fretting techniques will be considered) at this layer.
\par
The method employed here consisted of isolating the skin-tone portion of the cropped hand images \cite{pyimagesearch,seereality}
and passing them through a "skeletonization" filter \cite{skeleton} that blurs and subtracts the background (which is black)
inwards towards the center portion of the image until only a thin, strong line representing the fingers are left.
Hough Line detection was employed at this point in order to indentify line endings in an attempt to find the finger tip locations.
\par
A previous method that was intended to be adapted from several papers concerning the real-time tracking of hands and fingers
\cite{aslhand,handposes,fingertracking}.
However, the lack of available source code and complexity of the algorithms necessitated the simpler approach employed here.

\subsubsection{Note Production Music Model}
The final step is to interpret the hand models in context of the guitar models and determine when a plucking event occurs.
Several techniques may be employed here including frame averaging of these events to ensure that a plucking event
is correctly determined.
At this time, a several frame averaging algorithm will be employed leveraging the tempo of plucking events in context of 
the overall time signature determined by analysis of the events, with care taken to ensure abrupt time signature changes
are tracked accurately.
This will also ensure that the human-readable output of this final layer makes sense in context of the music that it is
analyzing, such that accuracy of the results obtained are at a level as to be useful to the end-user.
The output at this layer will either be directly printed on the screen (as a rolling queue of note events),
or piped to standard output such that the user can collect these events into a tablature file.

\subsection{Development Environment}
Initial prototyping work will be done using Python and OpenCV.
The layers are defined as individual modules such that accuracy of each individual stage can be verified as the design progresses.
Additional training modules for the Cascade Classifiers are located in an adjacent environment
and links to the XML output are maintained such that these classifiers are a part of the configuration of the final product.
%As the project progresses an implementation in C++ will be done in order to substantially optimize the processing of the utility.
The performance goal for this application would be on order of the running time of the video.
A system that can operate in real time would be considered succesful and open up a larger array of possibilities and use cases.
%\par
%The system will be constructed as a command line utility;
%either operating on an input video file of a specific format, with the option to
%print out the resultant tabulature to standard output (which may be piped to a file)
%or printing in-frame for use as a teaching aid.
%For verification, the obtained output of the program using the standard output method
%will be saved to a file and compared using an algorithm that will compare
%to a corresponding desired output file that will be constructed by hand for the input video.

\subsection{Evaluation}
Results will be prototyped and verified via a small collection of ~1-2 min video files
(5 total) with minor differences in lighting and clothing choice.
%The system will be vetted under a few different circumstances:
%standard performer, lighting, and instruments (20),
%standard enviroment with different fretting techniques (10),
%different lighting conditions (10),
%different instruments (5),
%and different performers (5).
%The system will be designed to adequetely handle these different circumstances
%without requiring enviroment-specific processing directives,
%except in the case of the different instruments/performers,
%where application-specific classifier files will be used.
\par
Each intermediate layer of the overall program will be vetted using simple criteria
for accuracy, which are as follows:
\begin{itemize}
    \item Location Classifiers: Presence of located object
    \item Guitar Model: Number of identified strings and frets
    \item Hand Model: Number of identified finger tips
\end{itemize}
Additionally, manual verification and quality determinations will be made by the author by reviewing
the quality of each layer using visual representations of the processing performed by each layer using the
set of videos provided.
\par
For the final system, an extended verification methodology will be employed against the desired output via the following:
\begin{itemize}
    \item Each note occurence in the desired output will be matched to a corresponding note in the obtained output
    \item If an incorrect note occurs in the obtained output, the algorithm will check ahead to see if further
        matches exist.
    \item When two sections of correct notes occur surrounding a section of incorrect notes, those will
        be analyzed to obtain the difference (missing note, added note or wrong note).
    \item The output correctness factor will be number of matches minus the number of additions and misses
        over the percentage of notes in the desired output (floored at zero percent correct).
\end{itemize}
The video files will be reviewed prior to testing and the desired output will be synthesized by hand.
\par
The overall goal for correctness would be 90\%.
This is an agressive goal for a computer vision application,
especially for one measuring the location of fingers on a hand.
Additionally, the obtained final tablature file can be converted to midi representation
for qualitative feedback using a conversion utility such as \cite{8notes}.
