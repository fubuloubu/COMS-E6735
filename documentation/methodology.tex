\subsection{Overview}
The \project Project is designed using a layered approach where each successive layer
uses the previous layer(s) in order to calculate a result.
The following is a description of these layers.
\subsubsection{Guitar Localization}
The first layer identifies the location of the guitar within the video frame.
Successful guitar localization should be able to identify many different types of
guitar shapes, body styles, colors, lengths, etc., as well as being able to account for
their orientations in the frame, in-plane angle to the camera, and distance away from the camera.
In practice that makes guitar localization a very difficult task.
\par
The original algorithm that was attempted was to train a Cascade Classifier using imagery obtained
of the guitar that was to be identified in a variety of lighting conditions and orientations using
the same camera that would be capturing the scenes to be analyzed by further layers.
After initial difficulty in training a Classifier that would perform acceptably under this circumstances
it was decided that a different algorithm should be explored that might not only be more accurate,
but be more generally applicable to different situations and contexts, as well as more user friendly.
After some investigation, a Binary Detector matching algorithm was chosen due to it's adaptable nature.
Initial algorithmic setup used a stock photo of the guitar to be identified as the baseline matching
object, with the idea that different guitars could be identified by simply supplying a different stock
photo for the baseline matching object.
The algorithm could also be refined such that a set of generic descriptor features are identified such
that this layer can identify many different body styles for a specific type of guitar.
For the purposes of this project, a single baseline descriptor object should be adequete for detection.
\par
This method has potential complications that could lead to difficulty in locating the guitar provided.
Not only does the guitar localization layer need to identify and locate the provided guitar,
but will also need to recognize these guitars in context of the players who play them.
This means the algorithm should be tuned in order to recognize the guitar despite player's these obscuring features.
Additionally, the location of the hands can vary drastically as the player moves up and down the neck,
and different picking/plucking/strumming techniques at different orientations
means this layer has many factors to account for.
This means that the Detector algorithm must define use sufficient amount of detector features
such that the overlaid hands and clothing do not interfere with the operation of the Detector.
The robustness of this layer could be improved by leveraging a motion filter such as a Kalman Filter
in order to smooth the output.

\subsubsection{Guitar Modeling}
The second layer creates a model of the relevant aspects of the guitar we are interested in.
This means we need to find the locations of all the frets and strings of the guitar,
as well as identifying the picking and fretting areas of the guitar.
The frets and strings will be identified using a threshold technique leveraging the fact that (for most instruments)
these features reflect light sources much more significantly than the guitar body and neck.
Once the thresholding is applied, Hough Line detection using a relatively large length threshold to avoid excessive
false positives is employed in order to isolate the sharply defined lines of the strings from the rest of the picture.
A group of a sufficient number of these lines all going in the same direction are then fed into a clustering algorithm.
A Jenks natural breaks algorithm is used to cluster these lines by their parallel offset and the resulting groups are
grouped into one representative line (combining lengths) for that string.
\par
Once the strings are isolated and determined to be correct, the next stage is to find the frets.
The frets are harder to locate given that they are partly occluded by the strings and that they are shorter in length.
However, leveraging the fact that they are perpindicular to the strings, Hough Line detection using a shorter length
threshold can be filtered down to the lines in that direction and bounded inside an area determined using the strings.
They will then be grouped according to an estimation of how many frets were identified.
\par
Under ideal conditions, localizing the fret and string locations should only need to be done once in order to obtain
all the information necessary to further parse the frame in further layers. 
However, the reality is that performers tend to move the instrument at least subtlely while performing, and additionally
move their hands significantly which obscuring parts of the regions of interest, therefore the guitar model will need to
be partially estimated until data is available to correct the obscured features.
Thankfully, these locations and spacings are about the only standardized attributes of a guitar, so localization of the rest
of the features can be done with minimal hassle given all the strings are identified and a sufficient number of contigious,
visible frets available for estimation.
The remaining frets can be estimated using the following recursive formula for fret location $l_{fi}$ using a scale length $l_S$:
\begin{equation}
    l_{fi} = \begin{cases}
        \frac{l_S}{17.817},                         & \text{if $i = 1$.} \\
        \frac{l_S - l_{fi-1}}{17.817} + l_{fi-1},   & \text{if $i > 1$.}
    \end{cases}
\end{equation}
Knowing the location several frets can also help back-calculate the scale length (e.g. the entire length of the string) since
usually the strings are difficult to fully capture due to lighting changes along the length of the string.

\subsubsection{Hand Localization}
The next layer is to find the location of both fretting hand and plucking hand in context of the guitar.
This layer can be more simplistic since the hands should be easy to find once the guitar is located.
After parsing the guitar orientation and cutting the frame in half along the body/neck joint,
the two segments should each contain a hand, thereby simplifying the search.
A standard skin detection algorithm is employed, assuming Caucasian skin tone\cite{pyimagesearch,seereality,skinsurvey},
in order to find two large flesh-toned shapes within the segmented frames provided from analysis of
the previous layer.
A threshold is used to determine if the detected hand is large enough to be considered realistic for the next layer.

\subsubsection{Hand Modeling}
From the hand location stage we now need to identify the players fingering and plucking points
by fitting a simplified hand model to each hand image obtained prior and obtaining the fingertip locations.
For the purposes of this project, we are using a bass guitar so we are only interested in two points of the plucking hand
(index and middle finger, only two finger plucking techniques will be considered)
and four points of the fretting hand (no thumb fretting techniques will be considered) in this layer.
The method employed here consisted of isolating the skin-tone portion of the cropped hand images
and passing them through a "skeletonization" filter \cite{skeleton} that blurs and subtracts the background (which is black)
inwards towards the center portion of the image until only a thin, strong line representing the fingers are left.
Hough Line detection was employed at this point in order to indentify line endings in an attempt to find the finger tip locations.
\par
A previous method that was intended to be adapted from several papers concerning the real-time tracking of hands and fingers
\cite{aslhand,handposes,fingertracking}.
However, the lack of available source code and complexity of the algorithms necessitated a simpler approach employed here.

\subsubsection{Note Production Music Model}
The final step is to interpret the hand models in context of the guitar model and determine when a plucking event occurs.
A several frame averaging algorithm will be employed leveraging the tempo of plucking events in context of the overall time
signature determined by analysis of the events, with care taken to ensure abrupt time signature changes are tracked accurately.
This will also ensure that the human-readable output of this final layer makes sense in context of the music that it is
being analyzed, such that accuracy of the results obtained are at a level useful to the end-user.
The output at this layer will either be directly printed on the screen (as a rolling queue of note events),
or piped to standard output such that the user can collect these events into a tablature file.

\subsection{Development Environment}
Initial prototyping work will be done using Python and OpenCV\cite{opencv}.
The layers are defined as individual modules such that accuracy of each individual stage can be verified as the design progresses.
Additional files required for proper implementation (training files, stock photos) are located in the adjacent environment.
As the project progresses an implementation in C++ will be done in order to substantially optimize the processing of the utility.
The performance goal for this application would be on order of the running time of the video.
A system that can operate in real time would be considered succesful and open up a larger array of possibilities and use cases.
%\par
%The system will be constructed as a command line utility;
%either operating on an input video file of a specific format, with the option to
%print out the resultant tabulature to standard output (which may be piped to a file)
%or printing in-frame for use as a teaching aid.
%For verification, the obtained output of the program using the standard output method
%will be saved to a file and compared using an algorithm that will compare
%to a corresponding desired output file that will be constructed by hand for the input video.

\subsection{Evaluation}
Results will be prototyped and verified via a small collection of ~1-2 min video files
(50 total) with minor differences in lighting, clothing choice, and playing style.
%The system will be vetted under a few different circumstances:
%standard performer, lighting, and instruments (20),
%standard enviroment with different fretting techniques (10),
%different lighting conditions (10),
%different instruments (5),
%and different performers (5).
%The system will be designed to adequetely handle these different circumstances
%without requiring enviroment-specific processing directives,
%except in the case of the different instruments/performers,
%where application-specific classifier files will be used.
\par
Each intermediate layer of the overall program will be vetted using simple criteria
for accuracy, which are as follows:
\begin{itemize}
    \item Object Location: Presence of located object of a correct size
    \item Guitar Model: Number of identified strings and frets
    \item Hand Model: Number of identified finger tips
\end{itemize}
Additionally, manual verification and quality determinations will be made by the author by reviewing
the quality of each layer using visual representations of the processing performed by each layer against the
set of videos provided.
\par
For the final system, an extended verification methodology will be employed against the desired output via the following:
\begin{itemize}
    \item Each note occurence in the desired output will be matched to a corresponding note in the obtained output
    \item If an incorrect note occurs in the obtained output, the algorithm will check ahead to see if further
        matches exist.
    \item When two sections of correct notes occur surrounding a section of incorrect notes, those will
        be analyzed to obtain the difference (missing note, added note or wrong note).
    \item The output correctness factor will be number of matches minus the number of additions and misses
        over the percentage of notes in the desired output (floored at zero percent correct).
\end{itemize}
The video files will be reviewed prior to testing and the desired output will be synthesized by hand for use in differencing.
\par
The overall goal for correctness would be 95\%.
This is an agressive goal for a computer vision application,
especially for one measuring the location of fingers on a hand.
Additionally, the obtained final tablature file can be converted to midi representation
for qualitative feedback using a conversion utility such as \cite{8notes}.
